{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2851194",
   "metadata": {
    "id": "f2851194"
   },
   "source": [
    "## GridWorld\n",
    "Ph.D Leonarod A, Espinosa, M.Sc Andrej Scherbakov-Parland, BIT Kristoffer Kuvaja Adolfsson\n",
    "\n",
    "### Bibliography:\n",
    "\n",
    "* Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n",
    "http://incompleteideas.net/book/bookdraft2017nov5.pdf  (chapter 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1f756d",
   "metadata": {
    "id": "8f1f756d"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07eafa2",
   "metadata": {
    "id": "d07eafa2"
   },
   "outputs": [],
   "source": [
    "# Utilits\n",
    "def printV(V, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):\n",
    "            state = grid.m * idx + idy\n",
    "            print('%.2f' % V[state], end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')\n",
    "\n",
    "def printPolicy(policy, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):\n",
    "            state = grid.m * idx + idy\n",
    "            if state in grid.stateSpace:\n",
    "                string = ''.join(policy[state])\n",
    "                print(string, end='\\t')\n",
    "            else:\n",
    "                print('', end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')\n",
    "\n",
    "def printQ(Q, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):\n",
    "            state = grid.m * idx + idy\n",
    "            if state != grid.m * grid.n - 1:\n",
    "                vals = [np.round(Q[state,action], 5) for action in grid.possibleActions]\n",
    "                print(vals, end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')\n",
    "\n",
    "def sampleReducedActionSpace(grid, action):\n",
    "    actions = grid.possibleActions[:]\n",
    "    actions.remove(action)\n",
    "    sample = np.random.choice(actions)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffbb837",
   "metadata": {
    "id": "0ffbb837"
   },
   "outputs": [],
   "source": [
    "class WindyGrid(object):\n",
    "    def __init__(self, m, n, wind):\n",
    "        self.grid = np.zeros((m,n))                            # representation of the grid\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.stateSpace = [i for i in range(self.m*self.n)]\n",
    "        self.stateSpace.remove(28)                              # Terminal state\n",
    "        self.stateSpacePlus = [i for i in range(self.m*self.n)] # State space + terminal state\n",
    "        self.actionSpace = {'U': -self.m, 'D': self.m,\n",
    "                            'L': -1, 'R': 1}\n",
    "        self.possibleActions = ['U', 'D', 'L', 'R']\n",
    "        self.agentPosition = 0\n",
    "        self.wind = wind\n",
    "\n",
    "    def isTerminalState(self, state):\n",
    "        return state in self.stateSpacePlus and state not in self.stateSpace\n",
    "\n",
    "    def getAgentRowAndColumn(self):                               # position of agent\n",
    "        x = self.agentPosition // self.m\n",
    "        y = self.agentPosition % self.n\n",
    "        return x, y\n",
    "\n",
    "    def setState(self, state):\n",
    "        x, y = self.getAgentRowAndColumn()\n",
    "        self.grid[x][y] = 0\n",
    "        self.agentPosition = state\n",
    "        x, y = self.getAgentRowAndColumn()\n",
    "        self.grid[x][y] = 1\n",
    "\n",
    "    def offGridMove(self, newState, oldState):\n",
    "        # if we move into a row not in the grid\n",
    "        if newState not in self.stateSpacePlus:\n",
    "            return True\n",
    "        # if we're trying to wrap around to next row\n",
    "        elif oldState % self.m == 0 and newState  % self.m == self.m - 1:\n",
    "            return True\n",
    "        elif oldState % self.m == self.m - 1 and newState % self.m == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Include wind stenght.\n",
    "    def step(self, action):\n",
    "        agentX, agentY = self.getAgentRowAndColumn()\n",
    "        if agentX > 0:\n",
    "            resultingState = self.agentPosition + self.actionSpace[action] + \\\n",
    "                            self.wind[agentY] * self.actionSpace['U']\n",
    "            if resultingState < 0: #if the wind is trying to push agent off grid\n",
    "                resultingState += self.m\n",
    "        else:\n",
    "            if action == 'L' or action == 'R':\n",
    "                resultingState = self.agentPosition + self.actionSpace[action]\n",
    "            else:\n",
    "                resultingState = self.agentPosition + self.actionSpace[action] + \\\n",
    "                            self.wind[agentY] * self.actionSpace['U']\n",
    "        #reward = -1 if not self.isTerminalState(resultingState) else 0\n",
    "        reward = -1\n",
    "        if not self.offGridMove(resultingState, self.agentPosition):\n",
    "            self.setState(resultingState)\n",
    "            return resultingState, reward, self.isTerminalState(resultingState), None\n",
    "        else:\n",
    "            return self.agentPosition, reward, self.isTerminalState(self.agentPosition), None\n",
    "\n",
    "    def reset(self):\n",
    "        self.agentPosition = 0\n",
    "        self.grid = np.zeros((self.m,self.n))\n",
    "        return self.agentPosition, False\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        print('------------------------------------------')\n",
    "        for row in self.grid:\n",
    "            for col in row:\n",
    "                if col == 0:\n",
    "                    print('-', end='\\t')\n",
    "                elif col == 1:\n",
    "                    print('X', end='\\t')\n",
    "            print('\\n')\n",
    "        print('------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216e193",
   "metadata": {
    "id": "4216e193"
   },
   "source": [
    "## First visit Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2387e21b",
   "metadata": {
    "id": "2387e21b"
   },
   "outputs": [],
   "source": [
    "def MC_first_visit(loop=500):\n",
    "\n",
    "    grid = WindyGrid(6,6, wind=[0, 0, 1, 2, 1, 0])\n",
    "    GAMMA = 1.0\n",
    "\n",
    "    policy = {}                              #  a dictionary that maps each\n",
    "    for state in grid.stateSpace:            #  state to the list of possible actions\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    V = {}                                   # Initialize our initial estimate of the value\n",
    "    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n",
    "        V[state] = 0\n",
    "\n",
    "    returns = {}                             #Initialize a dictionary that keeps a list\n",
    "    for state in grid.stateSpace:            #of the returns for each state.\n",
    "        returns[state] = []\n",
    "\n",
    "    for i in range(loop):                     # Loop over 500 games,\n",
    "        observation, done = grid.reset()     # resetting the grid and memory with each game.\n",
    "        memory = []                          # empty list to keep track of the states visited\n",
    "        statesReturns = []                   # and returns at each time step\n",
    "        if i % 100 == 0:                     # Just to know if the game is running.\n",
    "            print('starting episode', i)\n",
    "        while not done:                      # While the game isn't done\n",
    "            # attempt to follow the policy. In this case choose an action\n",
    "            # according to the random equiprobable strategy.\n",
    "            action = np.random.choice(policy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        # append terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0                                  # set G=0\n",
    "        last = True                            # initialize a Boolean to keep track of the visit to the last state\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:                                    # Skip the terminal state and append the set of states\n",
    "                statesReturns.append((state,G))      #  and returns to the statesReturns list.\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesReturns.reverse()                  # to ge it in chronological order\n",
    "        statesVisited = []                       # keep track of the visited states during the episode.\n",
    "        for state, G in statesReturns:\n",
    "            if state not in statesVisited:       # Iterate over the episode and see\n",
    "                returns[state].append(G)         # if each state has been visited before.\n",
    "                V[state] = np.mean(returns[state])\n",
    "                statesVisited.append(state)\n",
    "\n",
    "                #If it hasn't, meaning this is the agent's first visit, go ahead and append\n",
    "                #the returns to the returns dictionary for that state.\n",
    "                #Calculate the value function by taking the mean of the returns for that state, and finally,\n",
    "                #append that state to the list of statesVisited.\n",
    "    print(\"\\n\")\n",
    "    printV(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0c1eb",
   "metadata": {
    "id": "42c0c1eb"
   },
   "source": [
    "## Del 1:\n",
    "\n",
    "- Anv칛nd *first visit* Monte Carlo Metoden\n",
    "\n",
    "1. 칐ka vindstyrkan med en enhet.\n",
    "    - Hur 칛ndras slutv칛rdesfunktionen?\n",
    "    - Svar: Det har blivit tydligt sv친rare omgivning att l칬sa eftersom v칛rdena i genomsnitt 칬ver hela rutf칛ltet har minskat (st칬rre negativt v칛rde)\n",
    "\n",
    "2. Hur 칛ndras v칛rdefunktion om man 칛ndra gamma till:\n",
    "    - 洧=0.5 Svar: Alla v칛rden f칬rutom n친gra 칛ndrades till -2.0. \n",
    "    - 洧=0,9 Svar: Flesta v칛rden 칛ndrar till -10 / -9.99, om inte n친gon dera av dessa s친 andra v칛rden v칛ldigt n칛ra dessa.\n",
    "    - 洧=0,95 Svar: Flesta v칛rden 칛ndrar till -9.99, om inte n친gon dera av dessa s친 andra v칛rden v칛ldigt n칛ra dessa.\n",
    "\n",
    "\n",
    "3. Testa rutn칛tsv칛rlden i storlekarna:\n",
    "    - 8x8\n",
    "        - 츿ndra p친 vinden, vad h칛nder med v칛rdefunktion?\n",
    "            Svar: Med 칬kad vind och st칬rre rutn칛tverk 칬kar negativa v칛rdena.\n",
    "        - Prova med 洧=0,9, vad h칛nder med v칛rdefunktion?\n",
    "            Svar:Liknande som tidigare flesta v칛rden 칛ndrar till -10 / -9.99 eller n친got liknande v칛rde.\n",
    "    - 10x10\n",
    "        - 츿ndra p친 vinden, vad h칛nder med v칛rdefunktion?\n",
    "            Svar: F칬r mig s친 b칬rjade det dyka upp m친nga 0.0 v칛rden. Det kan bero p친 att jag lekte relativt mycket med vinden\n",
    "        - Prova med 洧=0,9, vad h칛nder med v칛rdefunktion?\n",
    "            Svar: Flesta v칛rden var 0.0, vissa -10 och vissa -9.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2885845a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2885845a",
    "outputId": "c7c3fe82-53f1-4af3-f226-f54b9e66d793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-1421.50\t-1421.38\t-1422.47\t-1395.80\t-1380.73\t-1358.29\t\n",
      "\n",
      "-1431.07\t-1421.06\t-1412.46\t-1422.03\t-1411.63\t-1334.13\t\n",
      "\n",
      "-1447.73\t-1440.39\t-1428.45\t-1422.99\t-1389.96\t-1261.53\t\n",
      "\n",
      "-1440.23\t-1442.32\t-1413.67\t-1436.38\t-1335.83\t-1067.55\t\n",
      "\n",
      "-1446.94\t-1456.22\t-1485.62\t-1350.55\t0.00\t-661.06\t\n",
      "\n",
      "-1422.18\t-1448.28\t-1407.92\t0.00\t-942.63\t-841.90\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# En k칬rning utan att ha 칛ndra vindstyrkan\n",
    "MC_first_visit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6fc670",
   "metadata": {
    "id": "bb6fc670"
   },
   "outputs": [],
   "source": [
    "# 츿ndrad vindstyrka med en enhet\n",
    "def MC_first_visitWindy(loop=500):\n",
    "\n",
    "    grid = WindyGrid(6,6, wind=[0, 0, 2, 3, 2, 0])\n",
    "    GAMMA = 1.0\n",
    "\n",
    "    policy = {}                              #  a dictionary that maps each\n",
    "    for state in grid.stateSpace:            #  state to the list of possible actions\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    V = {}                                   # Initialize our initial estimate of the value\n",
    "    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n",
    "        V[state] = 0\n",
    "\n",
    "    returns = {}                             #Initialize a dictionary that keeps a list\n",
    "    for state in grid.stateSpace:            #of the returns for each state.\n",
    "        returns[state] = []\n",
    "\n",
    "    for i in range(loop):                     # Loop over 500 games,\n",
    "        observation, done = grid.reset()     # resetting the grid and memory with each game.\n",
    "        memory = []                          # empty list to keep track of the states visited\n",
    "        statesReturns = []                   # and returns at each time step\n",
    "        if i % 100 == 0:                     # Just to know if the game is running.\n",
    "            print('starting episode', i)\n",
    "        while not done:                      # While the game isn't done\n",
    "            # attempt to follow the policy. In this case choose an action\n",
    "            # according to the random equiprobable strategy.\n",
    "            action = np.random.choice(policy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        # append terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0                                  # set G=0\n",
    "        last = True                            # initialize a Boolean to keep track of the visit to the last state\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:                                    # Skip the terminal state and append the set of states\n",
    "                statesReturns.append((state,G))      #  and returns to the statesReturns list.\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesReturns.reverse()                  # to ge it in chronological order\n",
    "        statesVisited = []                       # keep track of the visited states during the episode.\n",
    "        for state, G in statesReturns:\n",
    "            if state not in statesVisited:       # Iterate over the episode and see\n",
    "                returns[state].append(G)         # if each state has been visited before.\n",
    "                V[state] = np.mean(returns[state])\n",
    "                statesVisited.append(state)\n",
    "\n",
    "                #If it hasn't, meaning this is the agent's first visit, go ahead and append\n",
    "                #the returns to the returns dictionary for that state.\n",
    "                #Calculate the value function by taking the mean of the returns for that state, and finally,\n",
    "                #append that state to the list of statesVisited.\n",
    "    print(\"\\n\")\n",
    "    printV(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb5c2d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efb5c2d2",
    "outputId": "fd352119-c0a0-4f9a-8764-58b82f1a50fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-1647.03\t-1648.01\t-1634.85\t-1623.93\t-1605.77\t-1583.10\t\n",
      "\n",
      "-1640.19\t-1633.71\t-1631.79\t-1641.02\t-1584.98\t-1557.25\t\n",
      "\n",
      "-1638.22\t-1628.86\t-1626.52\t-1548.67\t-1602.21\t-1490.50\t\n",
      "\n",
      "-1655.13\t-1651.63\t-1638.54\t-1610.65\t-1573.94\t-1298.62\t\n",
      "\n",
      "-1651.37\t-1632.08\t-1606.01\t0.00\t0.00\t-729.89\t\n",
      "\n",
      "-1602.26\t-1607.19\t-1635.28\t0.00\t-1061.55\t-840.73\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visitWindy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf6e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 츿ndrad Gamma\n",
    "def MC_first_visitY05(loop=500):\n",
    "\n",
    "    grid = WindyGrid(6,6, wind=[0, 0, 2, 3, 2, 0])\n",
    "    GAMMA = 0.5\n",
    "\n",
    "    policy = {}                              #  a dictionary that maps each\n",
    "    for state in grid.stateSpace:            #  state to the list of possible actions\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    V = {}                                   # Initialize our initial estimate of the value\n",
    "    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n",
    "        V[state] = 0\n",
    "\n",
    "    returns = {}                             #Initialize a dictionary that keeps a list\n",
    "    for state in grid.stateSpace:            #of the returns for each state.\n",
    "        returns[state] = []\n",
    "\n",
    "    for i in range(loop):                     # Loop over 500 games,\n",
    "        observation, done = grid.reset()     # resetting the grid and memory with each game.\n",
    "        memory = []                          # empty list to keep track of the states visited\n",
    "        statesReturns = []                   # and returns at each time step\n",
    "        if i % 100 == 0:                     # Just to know if the game is running.\n",
    "            print('starting episode', i)\n",
    "        while not done:                      # While the game isn't done\n",
    "            # attempt to follow the policy. In this case choose an action\n",
    "            # according to the random equiprobable strategy.\n",
    "            action = np.random.choice(policy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        # append terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0                                  # set G=0\n",
    "        last = True                            # initialize a Boolean to keep track of the visit to the last state\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:                                    # Skip the terminal state and append the set of states\n",
    "                statesReturns.append((state,G))      #  and returns to the statesReturns list.\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesReturns.reverse()                  # to ge it in chronological order\n",
    "        statesVisited = []                       # keep track of the visited states during the episode.\n",
    "        for state, G in statesReturns:\n",
    "            if state not in statesVisited:       # Iterate over the episode and see\n",
    "                returns[state].append(G)         # if each state has been visited before.\n",
    "                V[state] = np.mean(returns[state])\n",
    "                statesVisited.append(state)\n",
    "\n",
    "                #If it hasn't, meaning this is the agent's first visit, go ahead and append\n",
    "                #the returns to the returns dictionary for that state.\n",
    "                #Calculate the value function by taking the mean of the returns for that state, and finally,\n",
    "                #append that state to the list of statesVisited.\n",
    "    print(\"\\n\")\n",
    "    printV(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7828aedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-1.99\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-1.97\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t0.00\t0.00\t-1.70\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t0.00\t-1.78\t-1.91\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visitY05()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f88abb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 츿ndrad Gamma\n",
    "def MC_first_visitY09(loop=500):\n",
    "\n",
    "    grid = WindyGrid(6,6, wind=[0, 0, 2, 3, 2, 0])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    policy = {}                              #  a dictionary that maps each\n",
    "    for state in grid.stateSpace:            #  state to the list of possible actions\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    V = {}                                   # Initialize our initial estimate of the value\n",
    "    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n",
    "        V[state] = 0\n",
    "\n",
    "    returns = {}                             #Initialize a dictionary that keeps a list\n",
    "    for state in grid.stateSpace:            #of the returns for each state.\n",
    "        returns[state] = []\n",
    "\n",
    "    for i in range(loop):                     # Loop over 500 games,\n",
    "        observation, done = grid.reset()     # resetting the grid and memory with each game.\n",
    "        memory = []                          # empty list to keep track of the states visited\n",
    "        statesReturns = []                   # and returns at each time step\n",
    "        if i % 100 == 0:                     # Just to know if the game is running.\n",
    "            print('starting episode', i)\n",
    "        while not done:                      # While the game isn't done\n",
    "            # attempt to follow the policy. In this case choose an action\n",
    "            # according to the random equiprobable strategy.\n",
    "            action = np.random.choice(policy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        # append terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0                                  # set G=0\n",
    "        last = True                            # initialize a Boolean to keep track of the visit to the last state\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:                                    # Skip the terminal state and append the set of states\n",
    "                statesReturns.append((state,G))      #  and returns to the statesReturns list.\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesReturns.reverse()                  # to ge it in chronological order\n",
    "        statesVisited = []                       # keep track of the visited states during the episode.\n",
    "        for state, G in statesReturns:\n",
    "            if state not in statesVisited:       # Iterate over the episode and see\n",
    "                returns[state].append(G)         # if each state has been visited before.\n",
    "                V[state] = np.mean(returns[state])\n",
    "                statesVisited.append(state)\n",
    "\n",
    "                #If it hasn't, meaning this is the agent's first visit, go ahead and append\n",
    "                #the returns to the returns dictionary for that state.\n",
    "                #Calculate the value function by taking the mean of the returns for that state, and finally,\n",
    "                #append that state to the list of statesVisited.\n",
    "    print(\"\\n\")\n",
    "    printV(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7aae826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-10.00\t-9.99\t-9.99\t-9.98\t-9.96\t-9.91\t\n",
      "\n",
      "-10.00\t-10.00\t-9.99\t-9.97\t-9.97\t-9.83\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-9.96\t-9.98\t-9.53\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-9.98\t-9.94\t-8.82\t\n",
      "\n",
      "-10.00\t-10.00\t-9.98\t0.00\t0.00\t-6.01\t\n",
      "\n",
      "-10.00\t-9.99\t-9.99\t0.00\t-6.88\t-7.16\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visitY09()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d50162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 츿ndrad Gamma\n",
    "def MC_first_visitY095(loop=500):\n",
    "\n",
    "    grid = WindyGrid(6,6, wind=[0, 0, 2, 3, 2, 0])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    policy = {}                              #  a dictionary that maps each\n",
    "    for state in grid.stateSpace:            #  state to the list of possible actions\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    V = {}                                   # Initialize our initial estimate of the value\n",
    "    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n",
    "        V[state] = 0\n",
    "\n",
    "    returns = {}                             #Initialize a dictionary that keeps a list\n",
    "    for state in grid.stateSpace:            #of the returns for each state.\n",
    "        returns[state] = []\n",
    "\n",
    "    for i in range(loop):                     # Loop over 500 games,\n",
    "        observation, done = grid.reset()     # resetting the grid and memory with each game.\n",
    "        memory = []                          # empty list to keep track of the states visited\n",
    "        statesReturns = []                   # and returns at each time step\n",
    "        if i % 100 == 0:                     # Just to know if the game is running.\n",
    "            print('starting episode', i)\n",
    "        while not done:                      # While the game isn't done\n",
    "            # attempt to follow the policy. In this case choose an action\n",
    "            # according to the random equiprobable strategy.\n",
    "            action = np.random.choice(policy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        # append terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0                                  # set G=0\n",
    "        last = True                            # initialize a Boolean to keep track of the visit to the last state\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:                                    # Skip the terminal state and append the set of states\n",
    "                statesReturns.append((state,G))      #  and returns to the statesReturns list.\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesReturns.reverse()                  # to ge it in chronological order\n",
    "        statesVisited = []                       # keep track of the visited states during the episode.\n",
    "        for state, G in statesReturns:\n",
    "            if state not in statesVisited:       # Iterate over the episode and see\n",
    "                returns[state].append(G)         # if each state has been visited before.\n",
    "                V[state] = np.mean(returns[state])\n",
    "                statesVisited.append(state)\n",
    "\n",
    "                #If it hasn't, meaning this is the agent's first visit, go ahead and append\n",
    "                #the returns to the returns dictionary for that state.\n",
    "                #Calculate the value function by taking the mean of the returns for that state, and finally,\n",
    "                #append that state to the list of statesVisited.\n",
    "    print(\"\\n\")\n",
    "    printV(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cef1e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.99\t-9.98\t-9.98\t-9.94\t-9.91\t-9.85\t\n",
      "\n",
      "-9.99\t-9.98\t-9.97\t-9.97\t-9.95\t-9.69\t\n",
      "\n",
      "-9.99\t-9.98\t-9.98\t-9.96\t-9.96\t-9.36\t\n",
      "\n",
      "-9.99\t-9.99\t-9.99\t-9.97\t-9.90\t-8.49\t\n",
      "\n",
      "-9.99\t-9.99\t-9.97\t0.00\t0.00\t-5.41\t\n",
      "\n",
      "-9.99\t-9.98\t-9.98\t0.00\t-6.93\t-7.09\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visitY095()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1382b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_first_visit8x8(loop=500):\n",
    "\n",
    "    grid = WindyGrid(8,8, wind=[0, 0, 2, 3, 2, 1, 0, 0])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    policy = {}                              #  a dictionary that maps each\n",
    "    for state in grid.stateSpace:            #  state to the list of possible actions\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    V = {}                                   # Initialize our initial estimate of the value\n",
    "    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n",
    "        V[state] = 0\n",
    "\n",
    "    returns = {}                             #Initialize a dictionary that keeps a list\n",
    "    for state in grid.stateSpace:            #of the returns for each state.\n",
    "        returns[state] = []\n",
    "\n",
    "    for i in range(loop):                     # Loop over 500 games,\n",
    "        observation, done = grid.reset()     # resetting the grid and memory with each game.\n",
    "        memory = []                          # empty list to keep track of the states visited\n",
    "        statesReturns = []                   # and returns at each time step\n",
    "        if i % 100 == 0:                     # Just to know if the game is running.\n",
    "            print('starting episode', i)\n",
    "        while not done:                      # While the game isn't done\n",
    "            # attempt to follow the policy. In this case choose an action\n",
    "            # according to the random equiprobable strategy.\n",
    "            action = np.random.choice(policy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        # append terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0                                  # set G=0\n",
    "        last = True                            # initialize a Boolean to keep track of the visit to the last state\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:                                    # Skip the terminal state and append the set of states\n",
    "                statesReturns.append((state,G))      #  and returns to the statesReturns list.\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesReturns.reverse()                  # to ge it in chronological order\n",
    "        statesVisited = []                       # keep track of the visited states during the episode.\n",
    "        for state, G in statesReturns:\n",
    "            if state not in statesVisited:       # Iterate over the episode and see\n",
    "                returns[state].append(G)         # if each state has been visited before.\n",
    "                V[state] = np.mean(returns[state])\n",
    "                statesVisited.append(state)\n",
    "\n",
    "                #If it hasn't, meaning this is the agent's first visit, go ahead and append\n",
    "                #the returns to the returns dictionary for that state.\n",
    "                #Calculate the value function by taking the mean of the returns for that state, and finally,\n",
    "                #append that state to the list of statesVisited.\n",
    "    print(\"\\n\")\n",
    "    printV(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05eaa868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-10.00\t-10.00\t-9.99\t-9.98\t-9.97\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-10.00\t-9.99\t-9.98\t-9.98\t-9.95\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-10.00\t-9.99\t-9.99\t-9.94\t-9.90\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-10.00\t0.00\t-9.93\t-9.67\t-9.80\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-10.00\t-7.73\t-7.07\t-9.12\t-9.54\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-10.00\t-9.46\t-9.27\t-9.33\t-9.45\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t0.00\t-6.82\t-8.89\t-9.33\t-9.54\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t0.00\t0.00\t-8.77\t-9.29\t-9.47\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visit8x8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d56a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_first_visit10x10(loop=500):\n",
    "\n",
    "    grid = WindyGrid(10,10, wind=[1, 1, 1, 2, 3, 3, 1, 1, 1, 0])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    policy = {}                              #  a dictionary that maps each\n",
    "    for state in grid.stateSpace:            #  state to the list of possible actions\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    V = {}                                   # Initialize our initial estimate of the value\n",
    "    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n",
    "        V[state] = 0\n",
    "\n",
    "    returns = {}                             #Initialize a dictionary that keeps a list\n",
    "    for state in grid.stateSpace:            #of the returns for each state.\n",
    "        returns[state] = []\n",
    "\n",
    "    for i in range(loop):                     # Loop over 500 games,\n",
    "        observation, done = grid.reset()     # resetting the grid and memory with each game.\n",
    "        memory = []                          # empty list to keep track of the states visited\n",
    "        statesReturns = []                   # and returns at each time step\n",
    "        if i % 100 == 0:                     # Just to know if the game is running.\n",
    "            print('starting episode', i)\n",
    "        while not done:                      # While the game isn't done\n",
    "            # attempt to follow the policy. In this case choose an action\n",
    "            # according to the random equiprobable strategy.\n",
    "            action = np.random.choice(policy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        # append terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0                                  # set G=0\n",
    "        last = True                            # initialize a Boolean to keep track of the visit to the last state\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:                                    # Skip the terminal state and append the set of states\n",
    "                statesReturns.append((state,G))      #  and returns to the statesReturns list.\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesReturns.reverse()                  # to ge it in chronological order\n",
    "        statesVisited = []                       # keep track of the visited states during the episode.\n",
    "        for state, G in statesReturns:\n",
    "            if state not in statesVisited:       # Iterate over the episode and see\n",
    "                returns[state].append(G)         # if each state has been visited before.\n",
    "                V[state] = np.mean(returns[state])\n",
    "                statesVisited.append(state)\n",
    "\n",
    "                #If it hasn't, meaning this is the agent's first visit, go ahead and append\n",
    "                #the returns to the returns dictionary for that state.\n",
    "                #Calculate the value function by taking the mean of the returns for that state, and finally,\n",
    "                #append that state to the list of statesVisited.\n",
    "    print(\"\\n\")\n",
    "    printV(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9140452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-10.00\t-10.00\t-9.99\t-9.98\t-9.98\t-9.95\t-9.92\t-9.84\t-9.64\t-9.32\t\n",
      "\n",
      "0.00\t0.00\t0.00\t0.00\t0.00\t-9.99\t-9.94\t-9.68\t-9.66\t-8.72\t\n",
      "\n",
      "0.00\t0.00\t0.00\t0.00\t0.00\t-10.00\t-9.28\t-9.77\t0.00\t-6.14\t\n",
      "\n",
      "0.00\t0.00\t0.00\t0.00\t0.00\t-10.00\t-10.00\t-7.08\t-8.83\t-7.74\t\n",
      "\n",
      "0.00\t0.00\t0.00\t0.00\t0.00\t-10.00\t-7.30\t-9.76\t-5.71\t-7.67\t\n",
      "\n",
      "0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t-10.00\t-8.14\t-8.46\t-8.28\t\n",
      "\n",
      "0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t-10.00\t-8.01\t-8.39\t\n",
      "\n",
      "0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t-7.39\t-8.67\t\n",
      "\n",
      "0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t-10.00\t-9.31\t\n",
      "\n",
      "0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t-8.95\t-9.24\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visit10x10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5ffb9",
   "metadata": {
    "id": "2be5ffb9"
   },
   "source": [
    "## Exploring Start Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428bf8de",
   "metadata": {
    "id": "428bf8de"
   },
   "outputs": [],
   "source": [
    "def MC_exploring_starts():\n",
    "    grid = WindyGrid(6,6, wind=[0, 0, 1, 2, 1, 0])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    # Initialize Q, returns, and pairs visited\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    # initialize a random policy\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 50000 == 0:\n",
    "            print('starting episode', i)\n",
    "        statesActionsReturns = []\n",
    "        observation = np.random.choice(grid.stateSpace)\n",
    "        action = np.random.choice(grid.possibleActions)\n",
    "        grid.setState(observation)\n",
    "        observation_, reward, done, info = grid.step(action)\n",
    "        memory = [(observation, action, reward)]\n",
    "        steps = 1\n",
    "        while not done:\n",
    "            action = policy[observation_]\n",
    "            steps += 1\n",
    "            observation, reward, done, info = grid.step(action)\n",
    "            if steps > 15 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation_, action, reward))\n",
    "            observation_ = observation\n",
    "\n",
    "        # append the terminal state\n",
    "        memory.append((observation_, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action, G))\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesActionsReturns.reverse()\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.argmax(values)\n",
    "                policy[state] = grid.possibleActions[best]\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298a493",
   "metadata": {
    "id": "9298a493"
   },
   "source": [
    "## Del 2\n",
    "\n",
    "\n",
    "- Anv칛nd  *exploring starts* Monte Carlo Metoden\n",
    "\n",
    "1. 칐ka vindstyrkan med en enhet.\n",
    "    - Hur 칛ndras slutv칛rdesfunktionen?\n",
    "        Svar: p친 ett mindre drastiskt s칛tt en de tidigare MC funktionerna. Detta kan bero p친 att jag inte 칬kat vinden tillr칛ckligt men k칬rningstiden blir s친 drastistk mycket l칛ngre alltid vid en stor 칬kning\n",
    "\n",
    "\n",
    "2. Hur 칛ndras policyn om man 칛ndra gamma till:\n",
    "    - 洧=0.5 Svar: V칛rdena 칛ndrades till -1.99 eller n친got annat v칛rde d칛r n칛ra\n",
    "    - 洧=0,9 Svar: Det ursprungliga k칬rningen var i -0.9 s친 inget att till칛gga\n",
    "    - 洧=0,95 Svar: V칛rdena f칬rskj칬t sig till -8 v칛rden.\n",
    "\n",
    "\n",
    "3. Testa rutn칛tsv칛rlden i storlekarna:\n",
    "    - 8x8\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?\n",
    "             Svar: Policyn h친ller f칬rv친nansv칛rt bra fast칛n det 칛r ett st칬rre f칛lt och 칬kad vind\n",
    "        - Prova med 洧=0,9, vad h칛nder med policyn?\n",
    "             Svar: gamman var 0.9 d친 p친 f칬rra k칬rningen eftersom denna funktion presenterades ursprungligen med gamma 0.9\n",
    "             GAMMA: 1\n",
    "             Svar: Med gamma 1 klarar sig policyn s칛mre 칛n med 0.9\n",
    "    - 10x10\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?\n",
    "            Svar: Policyn klarar sig 칛nnu men jag gjorde vinden mycket v칛rre vilket resulterade i s칛mre resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42bdbc66",
   "metadata": {
    "id": "42bdbc66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-6.88523, -6.91967, -6.88524, -6.51754]\t[-6.53039, -6.55184, -6.90646, -6.12736]\t[-6.13409, -6.15225, -6.5386, -5.69549]\t[-5.69694, -5.70237, -6.13712, -5.21726]\t[-5.22435, -5.22, -5.70178, -4.68576]\t[-4.69032, -4.09514, -5.21859, -4.69472]\t\n",
      "\n",
      "[-6.88971, -6.86558, -6.88042, -6.53605]\t[-6.54246, -6.51596, -6.87907, -6.12734]\t[-6.13231, -6.13546, -6.53278, -5.69545]\t[-5.70735, -5.69795, -6.13115, -5.21711]\t[-5.22104, -5.22177, -5.6981, -4.68576]\t[-4.68998, -3.43903, -5.21919, -4.10001]\t\n",
      "\n",
      "[-6.88136, -6.86786, -6.86913, -6.51348]\t[-6.52953, -6.52218, -6.86688, -6.12607]\t[-6.12968, -6.12852, -6.53097, -5.69552]\t[-5.69675, -5.7084, -6.12729, -5.21711]\t[-5.21844, -4.68725, -5.6966, -4.09516]\t[-4.09814, -2.71001, -4.68859, -3.44232]\t\n",
      "\n",
      "[-6.86424, -6.8701, -6.87158, -6.51763]\t[-6.51658, -6.51471, -6.86896, -6.12652]\t[-6.12715, -6.12833, -6.51809, -5.69543]\t[-5.69799, -5.70045, -6.12822, -5.21739]\t[-5.21884, -4.09968, -5.69666, -3.43931]\t[-3.44071, -1.9, -4.09657, -2.71183]\t\n",
      "\n",
      "[-6.87554, -6.51847, -6.86433, -6.51396]\t[-6.51857, -6.1261, -6.86435, -6.12798]\t[-6.12706, -6.13046, -6.51823, -5.69605]\t[-5.69665, -5.69741, -6.12816, -4.68559]\t[0, 0, 0, 0]\t[-2.71173, -2.71176, -1.0, -1.90179]\t\n",
      "\n",
      "[-6.86803, -6.51696, -6.51689, -6.12608]\t[-6.51585, -6.13109, -6.51452, -5.69549]\t[-6.12712, -5.6966, -6.51452, -5.2171]\t[-5.69901, -5.21835, -6.12702, -4.0951]\t[-4.09662, -2.71336, -5.21703, -1.90059]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tD\tR\tR\t\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_starts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f92dc5",
   "metadata": {
    "id": "a7f92dc5"
   },
   "outputs": [],
   "source": [
    "def MC_exploring_startsWindy():\n",
    "    grid = WindyGrid(6,6, wind=[1, 0, 2, 2, 1, 0])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    # Initialize Q, returns, and pairs visited\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    # initialize a random policy\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 50000 == 0:\n",
    "            print('starting episode', i)\n",
    "        statesActionsReturns = []\n",
    "        observation = np.random.choice(grid.stateSpace)\n",
    "        action = np.random.choice(grid.possibleActions)\n",
    "        grid.setState(observation)\n",
    "        observation_, reward, done, info = grid.step(action)\n",
    "        memory = [(observation, action, reward)]\n",
    "        steps = 1\n",
    "        while not done:\n",
    "            action = policy[observation_]\n",
    "            steps += 1\n",
    "            observation, reward, done, info = grid.step(action)\n",
    "            if steps > 15 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation_, action, reward))\n",
    "            observation_ = observation\n",
    "\n",
    "        # append the terminal state\n",
    "        memory.append((observation_, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action, G))\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesActionsReturns.reverse()\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.argmax(values)\n",
    "                policy[state] = grid.possibleActions[best]\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcd0efc3",
   "metadata": {
    "id": "fcd0efc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-6.98022, -6.97757, -6.95203, -6.61318]\t[-6.62032, -6.51804, -6.9491, -6.13324]\t[-6.28536, -6.16384, -6.62673, -5.69719]\t[-5.71805, -5.77205, -6.1637, -5.21919]\t[-5.23357, -5.26041, -5.80792, -4.68572]\t[-4.71968, -4.09527, -5.22577, -4.71149]\t\n",
      "\n",
      "[-7.00309, -6.96762, -6.97697, -6.59739]\t[-6.61685, -6.53653, -6.97763, -6.12816]\t[-6.14266, -6.15194, -6.63328, -5.69616]\t[-5.74923, -5.74459, -6.13071, -5.21807]\t[-5.22731, -5.22551, -5.70289, -4.68977]\t[-4.69456, -3.43906, -5.23433, -4.1155]\t\n",
      "\n",
      "[-6.96811, -6.89149, -6.88306, -6.51623]\t[-6.53926, -6.5714, -6.89009, -6.12683]\t[-6.15804, -6.13126, -6.61634, -5.69637]\t[-5.72286, -5.72301, -6.1427, -5.21746]\t[-5.23097, -4.68991, -5.72072, -4.09639]\t[-4.1057, -2.71001, -4.70002, -3.44557]\t\n",
      "\n",
      "[-6.97308, -6.88024, -6.87827, -6.51517]\t[-6.52965, -6.58557, -6.87858, -6.13244]\t[-6.13551, -6.14375, -6.51887, -5.70031]\t[-5.70411, -5.70183, -6.13678, -5.21828]\t[-5.22631, -4.11896, -5.70307, -3.44222]\t[-3.4469, -1.9, -4.11057, -2.71173]\t\n",
      "\n",
      "[-6.8856, -6.91324, -6.92202, -6.56183]\t[-6.56755, -6.52424, -6.93033, -6.12794]\t[-6.14217, -6.18758, -6.52049, -5.69621]\t[-5.71739, -5.69923, -6.13649, -4.68654]\t[0, 0, 0, 0]\t[-2.71338, -2.71243, -1.0, -1.9]\t\n",
      "\n",
      "[-6.87088, -6.87749, -6.88538, -6.52721]\t[-6.5419, -6.5215, -6.88149, -6.12688]\t[-6.13317, -6.13199, -6.56456, -5.69617]\t[-5.70535, -5.21994, -6.17593, -4.10847]\t[-4.11081, -2.71181, -5.22263, -1.9]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\t\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_startsWindy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65981cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_exploring_startsY():\n",
    "    grid = WindyGrid(6,6, wind=[1, 0, 2, 2, 1, 0])\n",
    "    GAMMA = 0.95\n",
    "\n",
    "    # Initialize Q, returns, and pairs visited\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    # initialize a random policy\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 50000 == 0:\n",
    "            print('starting episode', i)\n",
    "        statesActionsReturns = []\n",
    "        observation = np.random.choice(grid.stateSpace)\n",
    "        action = np.random.choice(grid.possibleActions)\n",
    "        grid.setState(observation)\n",
    "        observation_, reward, done, info = grid.step(action)\n",
    "        memory = [(observation, action, reward)]\n",
    "        steps = 1\n",
    "        while not done:\n",
    "            action = policy[observation_]\n",
    "            steps += 1\n",
    "            observation, reward, done, info = grid.step(action)\n",
    "            if steps > 15 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation_, action, reward))\n",
    "            observation_ = observation\n",
    "\n",
    "        # append the terminal state\n",
    "        memory.append((observation_, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action, G))\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesActionsReturns.reverse()\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.argmax(values)\n",
    "                policy[state] = grid.possibleActions[best]\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77f0a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-8.68882, -8.77179, -8.70482, -8.07249]\t[-8.08603, -8.03202, -8.74434, -7.39811]\t[-7.47249, -7.40802, -8.09801, -6.73243]\t[-6.77196, -6.75022, -7.40541, -6.03424]\t[-6.03615, -6.05034, -6.74237, -5.29885]\t[-5.34361, -4.52474, -6.04158, -5.30106]\t\n",
      "\n",
      "[-8.69202, -8.68473, -8.67995, -8.07283]\t[-8.0979, -8.04789, -8.67413, -7.39526]\t[-7.40024, -7.41025, -8.09323, -6.73229]\t[-6.73427, -6.75824, -7.40516, -6.03377]\t[-6.03883, -6.03873, -6.74724, -5.29932]\t[-5.30405, -3.71003, -6.04175, -4.55424]\t\n",
      "\n",
      "[-8.68889, -8.63114, -8.62871, -8.02771]\t[-8.03538, -8.03184, -8.62889, -7.3976]\t[-7.40816, -7.4079, -8.07476, -6.73221]\t[-6.73967, -6.74476, -7.39757, -6.03347]\t[-6.03601, -5.31536, -6.73692, -4.52637]\t[-4.53048, -2.85258, -5.31396, -3.73454]\t\n",
      "\n",
      "[-8.67044, -8.63366, -8.63124, -8.02674]\t[-8.02792, -8.03656, -8.62881, -7.39533]\t[-7.41038, -7.40001, -8.03264, -6.73278]\t[-6.73686, -6.73743, -7.39756, -6.03325]\t[-6.04146, -4.54225, -6.73898, -3.71236]\t[-3.71586, -1.95003, -4.5328, -2.85581]\t\n",
      "\n",
      "[-8.63086, -8.63525, -8.63613, -8.02679]\t[-8.03404, -8.02899, -8.63539, -7.40191]\t[-7.40276, -7.3997, -8.0302, -6.73226]\t[-6.73419, -6.73421, -7.39755, -5.30369]\t[0, 0, 0, 0]\t[-2.85585, -2.85568, -1.0, -1.95332]\t\n",
      "\n",
      "[-8.62881, -8.63398, -8.63459, -8.03267]\t[-8.03332, -8.02943, -8.63561, -7.39647]\t[-7.39761, -7.40253, -8.029, -6.73225]\t[-6.73468, -6.04942, -7.40249, -4.5348]\t[-4.54007, -2.85655, -6.04821, -1.95]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\t\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_startsY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23d48ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_exploring_starts8x8():\n",
    "    grid = WindyGrid(8,8, wind=[1, 1, 0, 2, 2, 1, 0, 0])\n",
    "    GAMMA = 1\n",
    "\n",
    "    # Initialize Q, returns, and pairs visited\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    # initialize a random policy\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 50000 == 0:\n",
    "            print('starting episode', i)\n",
    "        statesActionsReturns = []\n",
    "        observation = np.random.choice(grid.stateSpace)\n",
    "        action = np.random.choice(grid.possibleActions)\n",
    "        grid.setState(observation)\n",
    "        observation_, reward, done, info = grid.step(action)\n",
    "        memory = [(observation, action, reward)]\n",
    "        steps = 1\n",
    "        while not done:\n",
    "            action = policy[observation_]\n",
    "            steps += 1\n",
    "            observation, reward, done, info = grid.step(action)\n",
    "            if steps > 15 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation_, action, reward))\n",
    "            observation_ = observation\n",
    "\n",
    "        # append the terminal state\n",
    "        memory.append((observation_, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action, G))\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesActionsReturns.reverse()\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.argmax(values)\n",
    "                policy[state] = grid.possibleActions[best]\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acae285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-10.09126, -10.09173, -10.11739, -9.0791]\t[-10.86893, -9.06456, -10.09606, -8.00301]\t[-8.04719, -7.00019, -9.74783, -9.13251]\t[-9.55446, -9.1577, -8.00704, -9.55004]\t[-9.09491, -9.36416, -9.08037, -8.11681]\t[-8.77592, -8.11161, -9.063, -7.06666]\t[-7.07723, -6.0017, -8.20066, -8.0093]\t[-8.01918, -7.00318, -7.09322, -8.00942]\t\n",
      "\n",
      "[-10.07639, -10.31187, -10.35106, -9.06863]\t[-9.04727, -9.05488, -10.22791, -8.00519]\t[-8.06286, -6.00014, -9.06438, -9.61685]\t[-9.46856, -9.14131, -8.14463, -9.10225]\t[-9.37232, -9.10549, -9.19112, -8.05617]\t[-8.11134, -8.113, -9.08033, -7.05877]\t[-7.07242, -5.00593, -8.10422, -7.12045]\t[-8.01886, -6.00098, -6.15207, -7.0096]\t\n",
      "\n",
      "[-10.13792, -10.14736, -10.08881, -9.01096]\t[-9.23631, -8.03828, -10.32259, -7.00748]\t[-7.10022, -5.00025, -8.05475, -9.08963]\t[-9.12264, -9.64384, -8.00762, -9.12831]\t[-9.09316, -9.31268, -9.21743, -8.04893]\t[-8.11139, -7.05353, -9.36018, -6.04664]\t[-6.04207, -4.00972, -7.12226, -6.01925]\t[-7.03792, -5.0007, -5.05554, -6.01971]\t\n",
      "\n",
      "[-10.33241, -9.15261, -9.03678, -8.01324]\t[-9.03622, -7.05691, -10.061, -6.00974]\t[-6.02017, -4.00022, -7.02836, -8.03668]\t[-9.20925, -9.0777, -7.00754, -9.29543]\t[0, 0, 0, 0]\t[-8.14188, -6.04998, -9.18049, -5.02961]\t[-5.07766, -3.00061, -6.06518, -5.0082]\t[-6.0189, -4.00054, -4.0393, -5.02013]\t\n",
      "\n",
      "[-10.03632, -8.0379, -8.09086, -7.00787]\t[-8.06662, -6.05024, -9.0459, -5.00229]\t[-5.02997, -3.0001, -6.0301, -7.05733]\t[-9.48476, -8.028, -6.00956, -9.25094]\t[-9.32495, -1.0, -9.06623, -7.05649]\t[-7.04746, -2.01069, -1.0, -4.02918]\t[-4.03609, -4.03019, -2.00013, -4.0106]\t[-5.00991, -5.0194, -3.0004, -4.01048]\t\n",
      "\n",
      "[-9.04591, -7.00975, -7.06589, -6.00387]\t[-7.05779, -5.06774, -8.01863, -4.00264]\t[-4.01049, -4.01016, -5.04403, -2.00009]\t[-9.06255, -7.00934, -5.00942, -1.0]\t[-9.1726, -2.0, -8.0281, -6.0462]\t[-6.03553, -3.01028, -2.0, -3.0]\t[-3.04132, -4.02293, -3.00214, -5.01049]\t[-4.00634, -5.08726, -4.01012, -5.02017]\t\n",
      "\n",
      "[-8.05764, -6.03931, -6.05293, -5.01531]\t[-6.00981, -4.04286, -7.04664, -3.00179]\t[-3.00176, -5.01014, -4.01047, -3.03182]\t[-8.02865, -2.01052, -4.01017, -2.0]\t[-1.0, -3.01068, -7.00966, -2.01138]\t[-2.00103, -3.02218, -3.0, -4.01021]\t[-4.02007, -4.03321, -3.01923, -5.09246]\t[-5.02108, -5.08295, -4.01313, -5.06588]\t\n",
      "\n",
      "[-7.01841, -5.05583, -5.0504, -4.01058]\t[-5.03051, -5.04912, -6.04736, -4.00698]\t[-4.02077, -5.0105, -5.06203, -4.0]\t[-7.00962, -3.0, -3.0, -3.01105]\t[-2.03335, -2.02227, -2.01116, -3.0]\t[-3.01007, -3.01101, -2.0, -4.01398]\t[-4.10805, -4.02315, -3.00264, -5.02807]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tD\tL\tR\tR\tD\tD\t\n",
      "\n",
      "R\tR\tD\tL\tR\tR\tD\tD\t\n",
      "\n",
      "R\tR\tD\tL\tR\tR\tD\tD\t\n",
      "\n",
      "R\tR\tD\tL\t\tR\tD\tD\t\n",
      "\n",
      "R\tR\tD\tL\tD\tL\tL\tL\t\n",
      "\n",
      "R\tR\tR\tR\tD\tL\tL\tU\t\n",
      "\n",
      "R\tR\tU\tR\tU\tU\tL\tL\t\n",
      "\n",
      "R\tR\tR\tD\tL\tL\tL\tL\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_starts8x8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "033e70c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_exploring_starts10x10():\n",
    "    grid = WindyGrid(10,10, wind=[2, 1, 1, 0, 3, 2, 2, 0, 0, 0])\n",
    "    GAMMA = 1\n",
    "\n",
    "    # Initialize Q, returns, and pairs visited\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    # initialize a random policy\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 50000 == 0:\n",
    "            print('starting episode', i)\n",
    "        statesActionsReturns = []\n",
    "        observation = np.random.choice(grid.stateSpace)\n",
    "        action = np.random.choice(grid.possibleActions)\n",
    "        grid.setState(observation)\n",
    "        observation_, reward, done, info = grid.step(action)\n",
    "        memory = [(observation, action, reward)]\n",
    "        steps = 1\n",
    "        while not done:\n",
    "            action = policy[observation_]\n",
    "            steps += 1\n",
    "            observation, reward, done, info = grid.step(action)\n",
    "            if steps > 15 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation_, action, reward))\n",
    "            observation_ = observation\n",
    "\n",
    "        # append the terminal state\n",
    "        memory.append((observation_, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action, G))\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesActionsReturns.reverse()\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.argmax(values)\n",
    "                policy[state] = grid.possibleActions[best]\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d66638b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-11.09406, -11.98148, -11.06884, -10.00631]\t[-10.07271, -11.33742, -11.05663, -9.00867]\t[-9.0294, -9.62717, -10.33788, -8.00081]\t[-8.04399, -9.05866, -9.35886, -7.00042]\t[-7.36094, -7.03047, -8.02942, -6.00069]\t[-6.03185, -6.53927, -7.04151, -5.00073]\t[-5.01637, -5.3627, -6.06386, -4.00008]\t[-4.01515, -3.01659, -5.11724, -3.00013]\t[-3.01559, -2.00006, -4.01623, -4.01571]\t[-4.0157, -3.01717, -3.00313, -4.01615]\t\n",
      "\n",
      "[-11.11339, -11.0705, -11.10983, -10.04411]\t[-10.02723, -10.284, -11.09818, -9.00338]\t[-9.05691, -9.07327, -10.24162, -8.00125]\t[-8.14684, -8.00567, -9.04329, -8.04515]\t[-8.05887, -7.00577, -8.0313, -8.60447]\t[-6.31235, -6.01532, -7.04721, -5.00058]\t[-5.04847, -5.03123, -6.04678, -4.00329]\t[-4.03272, -2.00119, -5.01612, -2.01664]\t[-3.01424, -1.0, -3.01667, -3.017]\t[-4.03236, -2.00674, -2.01719, -3.01698]\t\n",
      "\n",
      "[-11.54242, -11.08048, -11.08337, -10.01107]\t[-10.53858, -10.04232, -11.05645, -9.01173]\t[-9.23118, -10.05648, -10.19237, -9.0067]\t[-9.05854, -8.07988, -10.17826, -7.00123]\t[-7.03027, -7.13524, -8.07692, -6.00052]\t[-6.18561, -6.04802, -7.01478, -5.00159]\t[-5.02908, -5.01601, -6.15012, -4.00176]\t[-3.01598, -3.01622, -5.07111, -1.0]\t[0, 0, 0, 0]\t[-3.01586, -3.01637, -1.0, -2.01677]\t\n",
      "\n",
      "[-11.09574, -11.15887, -11.17066, -10.00813]\t[-10.00286, -11.04163, -11.30404, -10.12475]\t[-9.04606, -9.65028, -10.02768, -8.00382]\t[-8.10458, -8.08395, -9.53507, -7.00353]\t[-7.04575, -8.04631, -8.08522, -6.00433]\t[-6.01579, -6.04735, -8.2026, -5.00088]\t[-5.06225, -5.12091, -6.04526, -3.00171]\t[-2.03371, -4.01623, -4.01614, -2.0]\t[-1.0, -3.01638, -3.01661, -3.03285]\t[-2.0068, -4.01663, -2.03436, -3.01624]\t\n",
      "\n",
      "[-11.05343, -11.06512, -11.43125, -10.10774]\t[-10.08383, -10.42097, -11.04153, -9.03113]\t[-10.02892, -9.74255, -11.02355, -8.05768]\t[-8.04762, -8.17929, -9.80943, -7.00169]\t[-7.04631, -7.02949, -9.01427, -6.00345]\t[-6.17943, -6.02682, -7.02933, -5.0039]\t[-5.0382, -4.03202, -6.08557, -2.00064]\t[-3.03227, -5.03226, -3.03983, -3.00058]\t[-2.0, -4.01781, -4.01526, -4.01597]\t[-3.03432, -5.01509, -3.00062, -4.01587]\t\n",
      "\n",
      "[-11.05396, -11.66226, -12.0693, -11.01904]\t[-11.19992, -10.59943, -11.47144, -9.51087]\t[-9.12138, -9.0431, -10.08507, -8.01122]\t[-8.0122, -8.04096, -9.02508, -7.01254]\t[-8.28239, -7.06197, -8.04589, -6.00193]\t[-6.04635, -6.09076, -7.01382, -4.01099]\t[-5.03785, -3.32294, -6.01564, -3.00108]\t[-4.00157, -6.01476, -4.04013, -4.01643]\t[-3.00188, -5.15006, -5.01569, -5.03027]\t[-4.0015, -6.02979, -4.01667, -5.01618]\t\n",
      "\n",
      "[-11.01647, -12.05341, -11.37836, -10.09772]\t[-10.08734, -10.04213, -12.10265, -9.01499]\t[-9.85676, -9.27947, -10.58072, -8.14305]\t[-8.25979, -8.12889, -9.25412, -7.02061]\t[-7.06082, -7.01261, -8.02933, -6.00115]\t[-6.10862, -5.10598, -7.03115, -3.01016]\t[-4.04737, -4.10227, -6.07515, -4.00119]\t[-5.00586, -5.06703, -5.05521, -5.11365]\t[-4.0156, -6.12251, -6.02942, -6.01373]\t[-5.0009, -7.03074, -5.1183, -6.01242]\t\n",
      "\n",
      "[-11.54982, -11.66375, -11.88258, -10.38487]\t[-10.80768, -10.26626, -11.53015, -9.13054]\t[-9.01372, -9.04274, -10.05551, -8.01962]\t[-8.04481, -7.14513, -9.1129, -7.0592]\t[-7.03857, -7.06124, -8.02986, -6.00646]\t[-6.08637, -4.04167, -7.02958, -4.01693]\t[-3.00222, -5.03995, -5.07994, -5.01421]\t[-6.04424, -6.1749, -4.01162, -6.13153]\t[-5.14541, -7.1207, -5.02149, -7.01229]\t[-6.0, -8.0148, -6.10481, -7.01497]\t\n",
      "\n",
      "[-12.06614, -11.85929, -11.0142, -10.00959]\t[-10.02547, -10.05873, -11.85572, -9.01492]\t[-9.2986, -9.08736, -10.25398, -8.07016]\t[-8.21429, -6.01058, -9.14224, -6.30242]\t[-7.01286, -7.03638, -8.2213, -5.04341]\t[-5.0688, -5.0326, -7.01876, -5.00294]\t[-4.01458, -4.06927, -4.0797, -6.01498]\t[-5.085, -7.0353, -5.00773, -7.21749]\t[-6.16429, -8.07114, -6.01641, -8.01169]\t[-7.00158, -9.02776, -7.09974, -8.02888]\t\n",
      "\n",
      "[-11.39992, -11.00994, -11.38163, -10.21709]\t[-10.2636, -10.13624, -11.01361, -9.06922]\t[-9.02902, -8.08983, -10.04641, -7.06901]\t[-7.12943, -6.06865, -8.12145, -5.01198]\t[-7.01502, -7.0951, -8.0289, -4.00891]\t[-4.00854, -6.0059, -7.0876, -4.03569]\t[-5.00289, -5.11185, -5.06662, -5.06369]\t[-6.09874, -7.05708, -6.01165, -8.04897]\t[-7.14854, -8.07107, -7.00626, -9.01378]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tR\tR\tR\tD\tL\t\n",
      "\n",
      "R\tR\tR\tD\tD\tR\tR\tD\tD\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tR\tR\tR\t\tL\t\n",
      "\n",
      "R\tU\tR\tR\tR\tR\tR\tR\tU\tU\t\n",
      "\n",
      "R\tR\tR\tR\tR\tR\tR\tR\tU\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tR\tR\tU\tU\tU\t\n",
      "\n",
      "R\tR\tR\tR\tR\tR\tR\tU\tU\tU\t\n",
      "\n",
      "R\tR\tR\tR\tR\tR\tU\tL\tL\tU\t\n",
      "\n",
      "R\tR\tR\tD\tR\tR\tU\tL\tL\tU\t\n",
      "\n",
      "R\tR\tR\tR\tR\tU\tU\tL\tL\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_starts10x10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b42a6",
   "metadata": {
    "id": "054b42a6"
   },
   "source": [
    "## On-policy first visit Monte Carlo for $\\varepsilon$-soft policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba2dcfa7",
   "metadata": {
    "id": "ba2dcfa7"
   },
   "outputs": [],
   "source": [
    "def MC_without_exploring_starts():\n",
    "    grid = WindyGrid(6,6, wind=[0, 0, 1, 2, 1, 0])\n",
    "    GAMMA = 0.9\n",
    "    EPS = 0.4\n",
    "\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.actionSpace.keys():\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    for i in range(1000000):\n",
    "        statesActionsReturns = []\n",
    "        if i % 100000 == 0:\n",
    "            print('starting episode', i)\n",
    "        observation, done = grid.reset()\n",
    "        memory = []\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            if len(policy[observation]) > 1:\n",
    "                action = np.random.choice(policy[observation])\n",
    "            else:\n",
    "                action = policy[observation]\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        #append the terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action,G))\n",
    "            G = GAMMA*G + reward\n",
    "        statesActionsReturns.reverse()\n",
    "\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.random.choice(np.where(values==values.max())[0])\n",
    "                rand = np.random.random()\n",
    "                if rand < 1 - EPS:\n",
    "                    policy[state] = grid.possibleActions[best]\n",
    "                else:\n",
    "                    policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57f54bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-13.14286, -13.1429, -13.1428, -13.1428]\t[-13.54723, -13.55092, -13.54807, -13.54381]\t[-13.96813, -13.98302, -13.97508, -13.96125]\t[-14.40207, -14.42335, -14.42212, -14.43105]\t[-14.91357, -15.07811, -14.9474, -15.05488]\t[-15.73333, -16.53104, -15.63402, -15.49425]\t\n",
      "\n",
      "[-13.54366, -13.53334, -13.51398, -13.53008]\t[-13.90587, -13.92111, -13.90642, -13.9107]\t[-14.39515, -14.37905, -14.48687, -14.38349]\t[-15.99043, -16.07759, -16.02481, -15.59127]\t[-16.81417, -18.60355, -16.57081, -17.38503]\t[-15.91373, -27.07582, -16.06786, -16.1119]\t\n",
      "\n",
      "[-13.99758, -13.966, -13.97966, -13.97004]\t[-14.39928, -14.40361, -14.44512, -14.46418]\t[-14.98014, -15.00203, -15.45115, -15.0029]\t[-16.02382, -16.18954, -17.41623, -16.33156]\t[0, 0, -18.11211, 0]\t[0, -26.0, 0, -16.57081]\t\n",
      "\n",
      "[-14.44801, -14.38186, -14.46334, -14.43025]\t[-14.8865, -14.90154, -14.88642, -14.88931]\t[-15.49021, -15.42428, -15.4272, -15.58195]\t[-18.11211, -18.11211, -17.08458, -18.23262]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[-14.97562, -14.88089, -14.96551, -15.0714]\t[-15.48006, -15.56367, -15.46758, -15.48129]\t[-16.06786, -16.00755, -16.21098, -16.60731]\t[-17.3009, -17.87175, -17.3009, -17.3009]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[-15.47235, -15.42664, -15.46235, -15.47157]\t[-16.1155, -16.07693, -16.00885, -16.37966]\t[-21.76561, -16.82769, -16.68937, -16.76347]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "--------------------\n",
      "L\tR\tD\tU\tU\tD\t\n",
      "\n",
      "R\tU\tD\tD\tL\tD\t\n",
      "\n",
      "D\tD\tU\tU\tR\tL\t\n",
      "\n",
      "D\tR\tD\tL\tUDLR\tUDLR\t\n",
      "\n",
      "D\tL\tD\tR\t\tUDLR\t\n",
      "\n",
      "D\tL\tL\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_without_exploring_starts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88293b06",
   "metadata": {
    "id": "88293b06"
   },
   "source": [
    "## Del 3\n",
    "- Anv칛nd *without exploring starts* Monte Carlo Metoden\n",
    "\n",
    "1. 칐ka vindstyrkan med en enhet.\n",
    "    - Hur 칛ndras slutv칛rdesfunktionen?\n",
    "        Svar: F칬rv칛ntat klarar sig agenten s칛mre, men endo relativt bra\n",
    "\n",
    "2. Hur 칛ndras policyn om man 칛ndra gamma till:\n",
    "    - 洧=0.5 Svar: S친 gott som alla svar 칛r omkring -2 i v칛rde med vissa decimalers skillnad\n",
    "    - 洧=0,9 Svar: Relativt d친liga resultat med t.om. -26\n",
    "    - 洧=0,95 Svar: V칛ldigt d친liga resultat t.om. -36\n",
    "\n",
    "\n",
    "3. Testa rutn칛tsv칛rlden i storlekarna:\n",
    "    - 8x8\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?\n",
    "            Svar: Det d칬k upp v칛ldigt m친nga 0.00 v칛rden\n",
    "        - Prova med 洧=0,9, vad h칛nder med policyn?\n",
    "            Svar: Koden var igen ursprungligen med gamma 0.9 s친 jag j칛mf칬r ist칛llet med 1.0\n",
    "            1.0: Flera 0 v칛rden finns men den f칬rv친nande aspekten 칛r att gamma 1.0 fungerar otroligt d친ligt j칛mf칬rt med   andra parameter specifikationer\n",
    "    - 10x10\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?\n",
    "            Svar: Med en gamma p친 0.9 och relativt stark vind klarar sig policyn v칛ldigt bra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2dc828a",
   "metadata": {
    "id": "d2dc828a"
   },
   "outputs": [],
   "source": [
    "def MC_without_exploring_startsWind():\n",
    "    grid = WindyGrid(6,6, wind=[1, 0, 2, 2, 2, 0])\n",
    "    GAMMA = 0.9\n",
    "    EPS = 0.4\n",
    "\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.actionSpace.keys():\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    for i in range(1000000):\n",
    "        statesActionsReturns = []\n",
    "        if i % 100000 == 0:\n",
    "            print('starting episode', i)\n",
    "        observation, done = grid.reset()\n",
    "        memory = []\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            if len(policy[observation]) > 1:\n",
    "                action = np.random.choice(policy[observation])\n",
    "            else:\n",
    "                action = policy[observation]\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        #append the terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action,G))\n",
    "            G = GAMMA*G + reward\n",
    "        statesActionsReturns.reverse()\n",
    "\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.random.choice(np.where(values==values.max())[0])\n",
    "                rand = np.random.random()\n",
    "                if rand < 1 - EPS:\n",
    "                    policy[state] = grid.possibleActions[best]\n",
    "                else:\n",
    "                    policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1a3d19a",
   "metadata": {
    "id": "a1a3d19a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-13.14281, -13.1428, -13.1428, -13.14283]\t[-13.49254, -13.49264, -13.49455, -13.49205]\t[-13.89897, -13.90395, -13.90358, -13.90266]\t[-14.3826, -14.39528, -14.37911, -14.38983]\t[-15.07536, -15.06862, -14.87732, -14.98252]\t[-18.84225, -21.17242, -16.3337, -15.4252]\t\n",
      "\n",
      "[-14.33971, -14.47259, -14.47906, -14.54685]\t[-13.88, -13.88698, -13.89344, -13.88]\t[-14.34065, -14.33678, -14.33514, -14.33557]\t[-15.91373, 0, -15.91373, -23.42625]\t[-16.57081, -17.3009, -38.7226, -28.8449]\t[-15.91373, -28.91386, -15.91373, -26.0]\t\n",
      "\n",
      "[-14.79881, -15.33013, -15.61206, -15.10968]\t[-14.31111, -14.38637, -14.31111, -14.31111]\t[-14.79012, -14.79012, -14.79012, -14.82757]\t[0, -28.8449, 0, 0]\t[0, 0, 0, 0]\t[-49.4, -16.57081, 0, -45.46]\t\n",
      "\n",
      "[-18.04567, -16.97936, -18.74369, -15.32236]\t[-14.79012, -14.79012, -14.79012, -16.06838]\t[-15.32236, -15.32236, -25.26437, -15.32236]\t[0, 0, 0, 0]\t[-21.12772, 0, 0, 0]\t[-17.3009, 0, -20.01495, -19.01346]\t\n",
      "\n",
      "[-15.91373, -15.91373, -15.91373, -15.91373]\t[-15.32236, -15.65211, -15.32236, -20.29336]\t[-15.91373, 0, 0, -26.96041]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[-25.26437, -16.57081, -21.12772, -18.11211]\t[-15.91373, -15.91373, -15.91373, -18.11211]\t[0, 0, -19.01346, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "--------------------\n",
      "L\tR\tU\tL\tL\tL\t\n",
      "\n",
      "U\tU\tL\tD\tU\tL\t\n",
      "\n",
      "R\tL\tU\tU\tUDLR\tR\t\n",
      "\n",
      "R\tR\tU\tUDLR\tD\tD\t\n",
      "\n",
      "D\tU\tD\tUDLR\t\tUDLR\t\n",
      "\n",
      "D\tU\tD\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_without_exploring_startsWind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d746b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_without_exploring_startsY():\n",
    "    grid = WindyGrid(6,6, wind=[1, 0, 2, 2, 2, 0])\n",
    "    GAMMA = 0.95\n",
    "    EPS = 0.4\n",
    "\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.actionSpace.keys():\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    for i in range(1000000):\n",
    "        statesActionsReturns = []\n",
    "        if i % 100000 == 0:\n",
    "            print('starting episode', i)\n",
    "        observation, done = grid.reset()\n",
    "        memory = []\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            if len(policy[observation]) > 1:\n",
    "                action = np.random.choice(policy[observation])\n",
    "            else:\n",
    "                action = policy[observation]\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        #append the terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action,G))\n",
    "            G = GAMMA*G + reward\n",
    "        statesActionsReturns.reverse()\n",
    "\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.random.choice(np.where(values==values.max())[0])\n",
    "                rand = np.random.random()\n",
    "                if rand < 1 - EPS:\n",
    "                    policy[state] = grid.possibleActions[best]\n",
    "                else:\n",
    "                    policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0509f9a",
   "metadata": {
    "id": "b0509f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-28.96406, -28.96419, -28.96406, -28.0196]\t[-29.43589, -28.32441, -29.43587, -28.08225]\t[-30.02404, -30.02275, -30.02681, -28.0107]\t[-30.63582, -30.63629, -30.63689, -27.52973]\t[-31.1963, -31.19483, -31.20165, -26.51767]\t[-31.78255, -24.77834, -31.78318, -31.78622]\t\n",
      "\n",
      "[-30.46417, -30.45967, -30.45843, -30.46048]\t[-29.93366, -29.00832, -29.93308, -28.37951]\t[-30.47579, -29.16025, -30.48091, -28.34663]\t[-34.52065, -34.57104, -34.91663, -34.50956]\t[-33.3402, -33.29353, -33.27864, -33.3687]\t[-32.4034, -21.87538, -32.40396, -32.4036]\t\n",
      "\n",
      "[-31.01869, -31.03836, -31.07185, -31.00859]\t[-30.45524, -29.63744, -30.45524, -29.09742]\t[-29.556, -29.91686, -31.03014, -29.07811]\t[-35.8426, -36.65764, -35.1748, -35.80905]\t[-34.25323, -34.26584, -34.3494, -34.35001]\t[-33.0472, -17.28086, -33.05909, -33.05229]\t\n",
      "\n",
      "[-31.66505, -31.6393, -31.68358, -31.91991]\t[-31.00552, -30.47775, -31.00552, -29.72978]\t[-30.00729, -30.08109, -31.61282, -30.68598]\t[-37.01872, -36.64549, -36.5711, -36.56513]\t[-34.58759, -34.44352, -34.53738, -34.54434]\t[-33.74883, -9.93297, -33.75558, -33.74216]\t\n",
      "\n",
      "[-33.12019, -32.60019, -32.19448, -32.19448]\t[-31.68905, -30.63765, -31.58476, -31.71039]\t[-30.46106, -33.13549, -33.18254, -32.06629]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-34.46823, -19.13463, -1.0, -34.44642]\t\n",
      "\n",
      "[-32.83629, -37.46216, -37.30222, -34.6846]\t[-32.70549, -33.43183, -32.19448, -30.70697]\t[-32.83629, -30.54222, -32.83629, -32.83629]\t[0, 0, 0, 0]\t[-36.01344, -1.0, -35.99495, -36.04007]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tL\tR\tR\tD\t\n",
      "\n",
      "L\tR\tD\tD\tL\tD\t\n",
      "\n",
      "R\tR\tR\tL\tU\tD\t\n",
      "\n",
      "D\tU\tU\tR\tD\tD\t\n",
      "\n",
      "U\tD\tD\tUDLR\t\tL\t\n",
      "\n",
      "U\tR\tD\tUDLR\tD\tL\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_without_exploring_startsY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "935f3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_without_exploring_starts8x8():\n",
    "    grid = WindyGrid(8,8, wind=[1, 2, 0, 3, 2, 1, 0, 0])\n",
    "    GAMMA = 1\n",
    "    EPS = 0.4\n",
    "\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.actionSpace.keys():\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    for i in range(1000000):\n",
    "        statesActionsReturns = []\n",
    "        if i % 100000 == 0:\n",
    "            print('starting episode', i)\n",
    "        observation, done = grid.reset()\n",
    "        memory = []\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            if len(policy[observation]) > 1:\n",
    "                action = np.random.choice(policy[observation])\n",
    "            else:\n",
    "                action = policy[observation]\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        #append the terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action,G))\n",
    "            G = GAMMA*G + reward\n",
    "        statesActionsReturns.reverse()\n",
    "\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.random.choice(np.where(values==values.max())[0])\n",
    "                rand = np.random.random()\n",
    "                if rand < 1 - EPS:\n",
    "                    policy[state] = grid.possibleActions[best]\n",
    "                else:\n",
    "                    policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dd9fed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-75.99996, -75.99996, -76.0, -75.99999]\t[-74.99983, -74.9999, -74.99983, -74.99983]\t[-74.0, -73.99976, -74.0, -74.0]\t[-72.6184, -72.57633, -72.47348, -72.51166]\t[-70.60079, -70.64729, -70.725, -70.66357]\t[-69.72857, -69.58519, -69.85417, -69.56522]\t[-68.72222, -68.6129, -68.63158, -68.42857]\t[-67.22222, -67.28571, -67.33333, -67.36364]\t\n",
      "\n",
      "[-67.0, -69.0, -66.5, -69.0]\t[-71.98656, -71.98995, -71.99001, -71.98976]\t[-73.0, -73.0, -72.99965, -73.0]\t[-71.83545, -71.90227, -71.83784, -71.89681]\t[-68.0, -68.0, -62.0, -68.0]\t[-65.0, -64.25, -66.0, -65.0]\t[-67.5, -67.0, -67.66667, -68.0]\t[-66.66667, -65.625, -67.0, -66.0]\t\n",
      "\n",
      "[-68.0, -68.0, -68.0, -67.66667]\t[-70.82143, -70.83051, -70.81159, -70.80029]\t[-72.0, -72.0, -72.0, -72.0]\t[-71.0, -71.0, -71.0, -70.94704]\t[0, 0, -67.0, 0]\t[-55.5, -58.5, -63.0, -67.0]\t[-66.66667, -65.5, -68.0, -65.6]\t[-63.6, -64.22222, -67.0, -63.2]\t\n",
      "\n",
      "[-65.0, -67.0, -66.0, 0]\t[-69.77778, -69.7, -70.0, -70.0]\t[-71.0, -71.0, -71.0, -70.99697]\t[-69.98286, -69.98895, -70.0, -70.0]\t[0, 0, 0, 0]\t[-62.0, 0, 0, 0]\t[-65.0, -63.0, -63.0, -65.0]\t[-62.0, -58.5, -57.6, -64.0]\t\n",
      "\n",
      "[0, -66.0, 0, -65.0]\t[-69.0, -69.0, -69.0, -68.71429]\t[-70.0, -70.0, -70.0, -70.0]\t[-69.0, -69.0, -69.0, -69.0]\t[0, 0, -62.0, -54.0]\t[-57.0, 0, 0, -63.0]\t[-64.0, -60.0, -64.0, -63.0]\t[-52.0, 0, -63.0, -63.0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[-68.0, -68.0, -68.0, 0]\t[-69.0, -66.66667, -69.0, -65.8]\t[-52.0, -68.0, -68.0, -68.0]\t[0, 0, 0, 0]\t[0, 0, -55.0, 0]\t[-63.0, -61.0, 0, -53.0]\t[0, 0, -26.0, -52.0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[-55.0, 0, -67.0, 0]\t[-63.33333, -68.0, -68.0, -68.0]\t[-67.0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, -60.0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-67.0, -65.0, 0, -64.0]\t[0, 0, 0, -63.0]\t[0, 0, 0, 0]\t[-56.0, 0, 0, 0]\t[0, 0, -57.0, -59.0]\t\n",
      "\n",
      "--------------------\n",
      "U\tR\tD\tL\tU\tU\tR\tR\t\n",
      "\n",
      "L\tD\tD\tU\tR\tU\tD\tD\t\n",
      "\n",
      "D\tU\tR\tL\tD\tU\tD\tR\t\n",
      "\n",
      "L\tD\tR\tL\t\tR\tD\tD\t\n",
      "\n",
      "L\tU\tR\tU\tU\tR\tR\tU\t\n",
      "\n",
      "UDLR\tU\tL\tU\tUDLR\tU\tR\tD\t\n",
      "\n",
      "UDLR\tR\tU\tR\tUDLR\tUDLR\tD\tUDLR\t\n",
      "\n",
      "UDLR\tUDLR\tL\tD\tUDLR\tR\tU\tR\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_without_exploring_starts8x8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1b0ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_without_exploring_starts10x10():\n",
    "    grid = WindyGrid(10,10, wind=[2, 1, 2, 0, 3, 2, 2, 1, 1, 0])\n",
    "    GAMMA = 0.9\n",
    "    EPS = 0.4\n",
    "\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.actionSpace.keys():\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    for i in range(1000000):\n",
    "        statesActionsReturns = []\n",
    "        if i % 100000 == 0:\n",
    "            print('starting episode', i)\n",
    "        observation, done = grid.reset()\n",
    "        memory = []\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            if len(policy[observation]) > 1:\n",
    "                action = np.random.choice(policy[observation])\n",
    "            else:\n",
    "                action = policy[observation]\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        #append the terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action,G))\n",
    "            G = GAMMA*G + reward\n",
    "        statesActionsReturns.reverse()\n",
    "\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.random.choice(np.where(values==values.max())[0])\n",
    "                rand = np.random.random()\n",
    "                if rand < 1 - EPS:\n",
    "                    policy[state] = grid.possibleActions[best]\n",
    "                else:\n",
    "                    policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8f3a5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-13.1428, -13.14296, -13.14293, -13.1428]\t[-13.492, -13.49422, -13.49301, -13.49204]\t[-13.88403, -13.88006, -13.88095, -13.88214]\t[-14.31111, -14.31111, -14.33348, -14.31111]\t[-14.81366, -15.13659, -15.44825, -14.82317]\t[-15.47677, -15.47841, -15.63447, -15.82713]\t[-19.13893, -18.40202, -19.82583, -20.01495]\t[0, -33.26531, -21.12772, -41.914]\t[-26.0, 0, 0, -45.46]\t[0, 0, -49.4, 0]\t\n",
      "\n",
      "[-25.26437, 0, -23.73793, -26.96041]\t[0, 0, 0, 0]\t[-16.78235, -16.10446, -17.056, -15.42144]\t[-14.79012, -14.79012, -14.79012, -14.85619]\t[-15.41535, -17.25748, -16.88655, -16.06058]\t[0, 0, 0, -26.96041]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, -22.36414, 0]\t[-15.91373, -15.91373, -19.59596, -16.60731]\t[-15.32236, -15.32236, -15.32236, -15.32236]\t[-15.91373, -15.91373, -16.28013, -15.91373]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-19.46747, -16.57081, -26.0, -16.57081]\t[-15.91373, -16.37612, -16.95653, -30.93878]\t[-33.26531, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-49.4, 0, -21.12772, -17.3009]\t[-18.11211, 0, -16.57081, -16.57081]\t[0, -17.3009, 0, -25.26437]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "--------------------\n",
      "U\tU\tD\tR\tU\tU\tD\tU\tU\tU\t\n",
      "\n",
      "D\tUDLR\tU\tL\tU\tU\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "UDLR\tU\tD\tU\tR\tUDLR\tUDLR\tUDLR\t\tUDLR\t\n",
      "\n",
      "UDLR\tUDLR\tU\tL\tR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "UDLR\tUDLR\tD\tD\tU\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "UDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "UDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "UDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "UDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "UDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_without_exploring_starts10x10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a2a0d",
   "metadata": {
    "id": "4d1a2a0d"
   },
   "source": [
    "## Off-Policy Monte Carlo prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc7f57b8",
   "metadata": {
    "id": "dc7f57b8"
   },
   "outputs": [],
   "source": [
    "def MC_off_policy_prediction():\n",
    "    grid = WindyGrid(6,6, wind=[0,0,1,2,1,0])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    Q = {}\n",
    "    C = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state,action)] = 0\n",
    "            C[(state,action)] = 0\n",
    "\n",
    "    targetPolicy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        targetPolicy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n",
    "        behaviorPolicy = {}\n",
    "        for state in grid.stateSpace:\n",
    "            behaviorPolicy[state] = grid.possibleActions\n",
    "        memory = []\n",
    "        observation, done = grid.reset()\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = np.random.choice(behaviorPolicy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "        last = True\n",
    "        for (state, action, reward) in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                C[state,action] += W\n",
    "                Q[state,action] += (W / C[state,action])*(G-Q[state,action])\n",
    "                prob = 1 if action in targetPolicy[state] else 0\n",
    "                W *= prob/(1/len(behaviorPolicy[state]))\n",
    "                if W == 0:\n",
    "                    break\n",
    "            G = GAMMA*G + reward\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(targetPolicy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fc880be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "[-38.05781, -37.52833, -37.00381, -35.15851]\t[-37.84606, -35.37806, -36.54147, -36.96263]\t[-37.44392, -38.10604, -38.34291, -37.73372]\t[-32.34379, -37.74501, -37.72947, -36.41051]\t[-38.16075, -38.39015, -36.90478, -34.08695]\t[-38.9, -39.0271, -35.90988, -33.43134]\t\n",
      "\n",
      "[-34.88785, -37.37171, -37.39188, -33.55938]\t[-35.95484, -38.7313, -36.69029, -35.99513]\t[-32.48952, -32.20699, -35.7896, -39.97103]\t[-39.78629, -38.95528, -39.79015, -39.44922]\t[-40.17366, -39.81162, -40.03659, -39.30259]\t[-39.88603, -38.42122, -37.80349, -39.27991]\t\n",
      "\n",
      "[-33.36636, -37.36397, -39.21693, -33.76473]\t[-35.59385, -38.92496, -37.08644, -37.31776]\t[-35.40367, -39.53461, -39.4144, -38.73432]\t[-39.2927, -40.31266, -36.21248, -40.33113]\t[-40.38411, -40.01968, -39.35724, -39.29753]\t[-39.19433, -40.01608, -39.79852, -38.17689]\t\n",
      "\n",
      "[-28.74084, -38.57146, -38.17726, -36.74418]\t[-37.9957, -38.934, -36.05406, -38.80411]\t[-34.46779, -38.31229, -30.65532, -39.52081]\t[-32.14371, -39.82468, -40.27357, -41.13095]\t[-40.45076, -40.06188, -39.88912, -40.39648]\t[-39.70527, -39.88303, -39.39052, -38.29143]\t\n",
      "\n",
      "[-38.92981, -39.90596, -39.74761, -38.47699]\t[-38.52528, -39.70807, -39.44056, -38.31506]\t[-38.70519, -36.82125, -38.88068, -38.95799]\t[-39.67504, -39.52791, -40.84703, -37.85894]\t[0, 0, 0, 0]\t[-37.00502, -40.52179, -4.46413, -40.43687]\t\n",
      "\n",
      "[-39.76697, -40.43175, -39.4344, -39.93485]\t[-38.05774, -40.10078, -40.45931, -39.36584]\t[-38.78661, -36.48392, -39.2225, -38.84158]\t[0, 0, 0, 0]\t[-38.59696, -41.22142, -38.1488, -37.63224]\t\n",
      "\n",
      "--------------------\n",
      "D\tU\tD\tR\tU\tL\t\n",
      "\n",
      "L\tR\tL\tL\tR\tU\t\n",
      "\n",
      "R\tU\tR\tR\tU\tU\t\n",
      "\n",
      "D\tD\tR\tD\tR\tU\t\n",
      "\n",
      "D\tL\tR\tU\t\tD\t\n",
      "\n",
      "D\tL\tR\tU\tL\tR\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_off_policy_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b9cc6",
   "metadata": {
    "id": "e27b9cc6"
   },
   "source": [
    "## Del 4\n",
    "- Anv칛nd *off-policy prediction* Monte Carlo Metoden\n",
    "\n",
    "1. 칐ka vindstyrkan med en enhet.\n",
    "    - Hur 칛ndras slutv칛rdesfunktionen?\n",
    "        Svar: Medeltalet av negative v칛rden steg lite, men policyn klarar sig r칛tt v칛l och relativt of칬r칛ndrad\n",
    "\n",
    "2. Hur 칛ndras policyn om man 칛ndra gamma till:\n",
    "    - 洧=0.5 Svar: Policyn klarar sig v칛ldigt bra och r칬r sig emellan -8 - -30\n",
    "    - 洧=0,9 Svar: Policyns v칛rden ligger mellan -36 och -42\n",
    "    - 洧=0,95 Svar: Policyn ligger i medelv칛rde kring -42 - -44\n",
    "\n",
    "\n",
    "3. Testa rutn칛tsv칛rlden i storlekarna:\n",
    "    - 8x8\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?\n",
    "            Svar: v칛rdena r칬r sig mellan -52 och -54, s친 sv친rare blev det nog f칬r agenten\n",
    "        - Prova med 洧=0,9, vad h칛nder med policyn?\n",
    "            Svar: gamma 0,9 klarar sig b칛ttre 칛n 1\n",
    "    - 10x10\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?\n",
    "            Svar: Policyn klarar sig r칛tt bra, trots att f칛ltet 칛r st칬rre och vinden 칛r 칬kad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "406a1b42",
   "metadata": {
    "id": "406a1b42"
   },
   "outputs": [],
   "source": [
    "def MC_off_policy_predictionWindy():\n",
    "    grid = WindyGrid(6,6, wind=[2,0,2,3,2,0])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    Q = {}\n",
    "    C = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state,action)] = 0\n",
    "            C[(state,action)] = 0\n",
    "\n",
    "    targetPolicy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        targetPolicy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n",
    "        behaviorPolicy = {}\n",
    "        for state in grid.stateSpace:\n",
    "            behaviorPolicy[state] = grid.possibleActions\n",
    "        memory = []\n",
    "        observation, done = grid.reset()\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = np.random.choice(behaviorPolicy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "        last = True\n",
    "        for (state, action, reward) in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                C[state,action] += W\n",
    "                Q[state,action] += (W / C[state,action])*(G-Q[state,action])\n",
    "                prob = 1 if action in targetPolicy[state] else 0\n",
    "                W *= prob/(1/len(behaviorPolicy[state]))\n",
    "                if W == 0:\n",
    "                    break\n",
    "            G = GAMMA*G + reward\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(targetPolicy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a06c5",
   "metadata": {
    "id": "560a06c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "MC_off_policy_predictionWindy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455c795f",
   "metadata": {
    "id": "455c795f"
   },
   "outputs": [],
   "source": [
    "def MC_off_policy_predictionY():\n",
    "    grid = WindyGrid(6,6, wind=[2,0,2,3,2,0])\n",
    "    GAMMA = 0.95\n",
    "\n",
    "    Q = {}\n",
    "    C = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state,action)] = 0\n",
    "            C[(state,action)] = 0\n",
    "\n",
    "    targetPolicy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        targetPolicy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n",
    "        behaviorPolicy = {}\n",
    "        for state in grid.stateSpace:\n",
    "            behaviorPolicy[state] = grid.possibleActions\n",
    "        memory = []\n",
    "        observation, done = grid.reset()\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = np.random.choice(behaviorPolicy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "        last = True\n",
    "        for (state, action, reward) in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                C[state,action] += W\n",
    "                Q[state,action] += (W / C[state,action])*(G-Q[state,action])\n",
    "                prob = 1 if action in targetPolicy[state] else 0\n",
    "                W *= prob/(1/len(behaviorPolicy[state]))\n",
    "                if W == 0:\n",
    "                    break\n",
    "            G = GAMMA*G + reward\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(targetPolicy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcbbdf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "[-42.57625, -43.57025, -43.14464, -43.63798]\t[-44.22777, -43.54978, -43.4369, -43.8944]\t[-43.91264, -42.28521, -43.59771, -43.47755]\t[-43.9393, -43.80614, -43.99276, -43.97045]\t[-43.96838, -43.65026, -43.87809, -43.73913]\t[-43.90589, -43.65859, -43.83027, -43.68605]\t\n",
      "\n",
      "[-43.61045, -43.94145, -44.20668, -40.92329]\t[-43.81334, -42.76214, -39.7199, -44.04583]\t[-44.23892, -44.20638, -43.66344, -44.19882]\t[-43.75631, -43.48755, -43.24941, -43.81391]\t[-43.92868, -42.77372, -43.69055, -43.65318]\t[-43.75598, -43.84408, -43.65722, -43.64151]\t\n",
      "\n",
      "[-43.92818, -43.7478, -43.55722, -44.38002]\t[-44.11981, -44.15267, -43.28909, -43.90108]\t[-44.49204, -44.12212, -42.954, -43.45948]\t[-41.14323, -38.35, -41.49804, -42.5975]\t[-43.71235, -43.97342, -43.2782, -42.72402]\t[-44.18386, -43.48263, -43.56544, -43.82198]\t\n",
      "\n",
      "[-44.27784, -44.8645, -43.76423, -44.17511]\t[-44.56681, -44.464, -44.45598, -43.90697]\t[-43.53626, -43.81704, -44.16053, -43.69391]\t[-38.35467, -34.5913, -47.67185, -46.55649]\t[-44.82095, -43.65177, -42.53383, -43.85971]\t[-43.96614, -10.10334, -44.05365, -42.03919]\t\n",
      "\n",
      "[-42.51229, -44.21295, -43.39798, -42.52096]\t[-43.78615, -44.25616, -41.44412, -44.48649]\t[-44.97218, -45.26557, -42.10616, -42.26433]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-43.85461, -20.29119, -4.43567, -11.64679]\t\n",
      "\n",
      "[-38.128, -34.46857, -38.764, -43.60838]\t[-44.58225, -39.65221, -39.85439, -44.62057]\t[-43.5878, -41.63646, -35.88, -45.1821]\t[0, 0, 0, 0]\t[-43.22716, -4.39506, -40.41067, -41.2]\t\n",
      "\n",
      "--------------------\n",
      "D\tD\tL\tR\tU\tR\t\n",
      "\n",
      "R\tU\tD\tR\tL\tD\t\n",
      "\n",
      "D\tL\tR\tL\tR\tU\t\n",
      "\n",
      "D\tR\tU\tR\tR\tR\t\n",
      "\n",
      "R\tU\tU\tD\t\tL\t\n",
      "\n",
      "D\tU\tD\tD\tU\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_off_policy_predictionY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d16d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_off_policy_prediction8x8():\n",
    "    grid = WindyGrid(8,8, wind=[1,3,0,2,2,1,1,0])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    Q = {}\n",
    "    C = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state,action)] = 0\n",
    "            C[(state,action)] = 0\n",
    "\n",
    "    targetPolicy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        targetPolicy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n",
    "        behaviorPolicy = {}\n",
    "        for state in grid.stateSpace:\n",
    "            behaviorPolicy[state] = grid.possibleActions\n",
    "        memory = []\n",
    "        observation, done = grid.reset()\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = np.random.choice(behaviorPolicy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "        last = True\n",
    "        for (state, action, reward) in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                C[state,action] += W\n",
    "                Q[state,action] += (W / C[state,action])*(G-Q[state,action])\n",
    "                prob = 1 if action in targetPolicy[state] else 0\n",
    "                W *= prob/(1/len(behaviorPolicy[state]))\n",
    "                if W == 0:\n",
    "                    break\n",
    "            G = GAMMA*G + reward\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(targetPolicy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0360df75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "[-36.40153, -37.50964, -36.39348, -28.9872]\t[-37.92264, -35.64033, -35.63362, -33.35021]\t[-36.41845, -36.07152, -36.12156, -35.77309]\t[-37.36407, -37.19363, -37.08183, -30.19848]\t[-38.32023, -36.64848, -34.20683, -37.52381]\t[-38.96099, -38.76363, -38.68576, -37.51531]\t[-38.91062, -38.52913, -37.45711, -38.99749]\t[-39.00536, -39.27524, -39.48146, -39.0092]\t\n",
      "\n",
      "[-40.09905, -40.00812, -40.81561, -38.13333]\t[-29.30774, -33.53326, -38.745, -39.5257]\t[-37.23952, -39.04061, -37.16487, -37.23678]\t[-38.73169, -39.10804, -36.8967, -38.86727]\t[-40.24824, -41.32133, -40.05687, -41.78643]\t[-37.51248, -36.99053, -39.56139, -38.69565]\t[-38.94267, -36.19676, -40.54877, -38.88079]\t[-39.43835, -39.8209, -39.65912, -40.01672]\t\n",
      "\n",
      "[-42.13889, -38.76364, -45.59314, -43.01818]\t[-40.26338, -40.6778, -40.56389, -36.83279]\t[-39.9011, -38.92728, -39.948, -39.77168]\t[-40.49588, -40.61973, -39.7842, -40.62486]\t[-32.68571, -42.99325, -42.00432, -36.124]\t[-41.58857, -41.10788, -32.45517, -39.50509]\t[-40.05732, -39.3696, -39.96359, -40.49718]\t[-39.94004, -38.43234, -40.14237, -40.16574]\t\n",
      "\n",
      "[-44.72, -49.4, -49.4, -42.38737]\t[-40.5434, -37.09453, -41.19327, -39.88973]\t[-40.33487, -39.64383, -36.20434, -39.74616]\t[-40.23371, -38.25901, -39.92166, -40.48656]\t[0, 0, 0, 0]\t[-41.6, -39.37143, -26.0, -38.48]\t[-39.83804, -41.16133, -39.82956, -38.14338]\t[-39.7062, -38.39306, -39.78626, -39.76022]\t\n",
      "\n",
      "[-26.0, 0, 0, 0]\t[-39.33328, -39.46847, -41.38211, -39.05912]\t[-39.45226, -40.70381, -40.94778, -37.43999]\t[-40.76725, -38.859, -41.25757, -40.40709]\t[0, -3.08333, -44.72, -26.0]\t[-44.72, -1.9, -6.55556, -26.0]\t[-39.634, -40.804, -34.50909, -39.634]\t[-39.71663, -42.16874, -38.1152, -37.29575]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[-43.89926, -39.45818, -42.13429, -35.83153]\t[-40.415, -39.91277, -33.45142, -39.89608]\t[-37.91059, -26.0, -37.59442, -2.01523]\t[0, 0, 0, 0]\t[0, 0, 0, -26.0]\t[-33.2, -36.4, -17.06667, -26.0]\t[-42.36737, -42.11586, -36.4, -38.12444]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[-39.37143, -44.72, -41.66332, -26.0]\t[-41.23911, -39.96414, -42.22356, -39.37143]\t[-46.05714, -26.0, -26.0, -26.0]\t[0, 0, 0, 0]\t[-26.0, 0, 0, 0]\t[0, -26.0, -26.0, 0]\t[-33.8, -40.77895, -26.0, -40.4]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[-41.6, -44.72, 0, -44.72]\t[-46.05714, -43.29778, -42.1984, -26.0]\t[-26.0, -49.4, -41.6, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, -26.0, -44.72, 0]\t\n",
      "\n",
      "--------------------\n",
      "D\tR\tD\tD\tL\tU\tR\tL\t\n",
      "\n",
      "R\tD\tU\tU\tR\tU\tU\tR\t\n",
      "\n",
      "D\tU\tU\tD\tL\tR\tD\tR\t\n",
      "\n",
      "R\tD\tR\tR\t\tL\tD\tL\t\n",
      "\n",
      "R\tU\tL\tD\tU\tL\tD\tR\t\n",
      "\n",
      "U\tR\tD\tU\tL\tD\tR\tR\t\n",
      "\n",
      "U\tU\tU\tR\tD\tU\tU\tD\t\n",
      "\n",
      "R\tR\tU\tR\tU\tU\tD\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_off_policy_prediction8x8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1772026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_off_policy_prediction10x10():\n",
    "    grid = WindyGrid(10,10, wind=[1, 1,3,0,2,2,1,1,0, 1])\n",
    "    GAMMA = 0.9\n",
    "\n",
    "    Q = {}\n",
    "    C = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state,action)] = 0\n",
    "            C[(state,action)] = 0\n",
    "\n",
    "    targetPolicy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        targetPolicy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n",
    "        behaviorPolicy = {}\n",
    "        for state in grid.stateSpace:\n",
    "            behaviorPolicy[state] = grid.possibleActions\n",
    "        memory = []\n",
    "        observation, done = grid.reset()\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = np.random.choice(behaviorPolicy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "        last = True\n",
    "        for (state, action, reward) in reversed(memory):\n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                C[state,action] += W\n",
    "                Q[state,action] += (W / C[state,action])*(G-Q[state,action])\n",
    "                prob = 1 if action in targetPolicy[state] else 0\n",
    "                W *= prob/(1/len(behaviorPolicy[state]))\n",
    "                if W == 0:\n",
    "                    break\n",
    "            G = GAMMA*G + reward\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(targetPolicy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237c3f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "[-35.99384, -37.62612, -36.80307, -38.03425]\t[-35.74897, -37.84433, -38.1226, -33.35896]\t[-36.17176, -32.65382, -36.12827, -35.28551]\t[-36.89722, -38.11074, -37.6985, -39.0713]\t[-39.04348, -38.35821, -35.72594, -38.61525]\t[-35.05473, -38.00016, -34.98939, -37.83342]\t[-35.77162, -38.67482, -39.4914, -38.37358]\t[-38.10008, -39.34053, -38.77216, -38.72812]\t[-37.7216, -30.40062, -39.81335, -39.71349]\t[-39.61346, -38.33476, -39.87662, -39.18638]\t\n",
      "\n",
      "[-26.0, -44.72, -26.0, -45.46]\t[-39.06323, -40.09905, -38.78359, -33.2]\t[-39.88026, -37.81541, -39.54251, -37.70885]\t[-36.82179, -36.71122, -39.12847, -39.29959]\t[-39.52575, -38.17523, -39.85703, -39.99454]\t[-39.71937, -36.7989, -41.60576, -37.38569]\t[-26.0, -41.43176, -49.4, -45.46]\t[-36.97306, -39.57496, -40.24444, -40.00013]\t[-32.00314, -5.17355, -39.96474, -38.94218]\t[-39.61591, -40.13213, -37.65855, -39.6817]\t\n",
      "\n",
      "[0, 0, 0, -26.0]\t[-41.44578, -37.7, -26.0, -26.0]\t[-40.37126, -39.94136, -39.13819, -36.25693]\t[-37.7281, -37.61296, -39.16125, -37.74428]\t[-39.2609, -40.75382, -40.13591, -40.46822]\t[-33.2, -43.19172, -32.24, -40.18466]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, -26.0, 0]\t[-42.55181, -42.11733, 0, -26.0]\t[-39.75005, -40.46686, -40.64804, -41.10064]\t[-35.53551, -40.04498, -40.70864, -38.86788]\t[-40.30099, -39.92354, -40.59358, -39.76721]\t[-45.28381, -46.8, -41.6, -26.0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, -49.4]\t[-38.20948, -38.32744, -40.33747, -39.97092]\t[-38.5876, -41.16995, -41.29064, -39.73318]\t[-41.72106, -40.33867, -40.69186, -39.4871]\t[0, -26.0, -26.0, -26.0]\t[0, 0, 0, -26.0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-40.70361, -26.0, -26.0, -32.68571]\t[-41.5114, -41.46827, -39.70725, -41.42444]\t[-38.76364, -37.57029, -41.24085, -36.4]\t[0, -26.0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-26.0, -26.0, -39.07838, -26.0]\t[-36.4, -26.0, -36.37004, -38.91034]\t[-26.0, -26.0, -44.40727, -26.0]\t[0, 0, -26.0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-49.4, -26.0, -45.46, -45.46]\t[-43.60696, -45.46, -40.76081, -37.7]\t[-26.0, -26.0, 0, -49.4]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, -49.4, 0, -26.0]\t[-41.43176, 0, -49.4, -26.0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, -26.0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "--------------------\n",
      "R\tD\tR\tU\tR\tD\tR\tD\tU\tU\t\n",
      "\n",
      "U\tD\tU\tL\tR\tR\tR\tL\tU\tR\t\n",
      "\n",
      "L\tR\tD\tU\tU\tR\tR\tU\t\tL\t\n",
      "\n",
      "D\tU\tD\tR\tL\tR\tD\tR\tU\tD\t\n",
      "\n",
      "U\tR\tL\tL\tD\tU\tU\tU\tL\tD\t\n",
      "\n",
      "U\tU\tR\tL\tL\tD\tL\tL\tD\tR\t\n",
      "\n",
      "U\tD\tL\tR\tU\tR\tR\tD\tU\tU\t\n",
      "\n",
      "R\tL\tL\tD\tD\tD\tD\tU\tR\tR\t\n",
      "\n",
      "L\tD\tR\tL\tU\tL\tU\tU\tU\tU\t\n",
      "\n",
      "D\tD\tL\tL\tD\tU\tL\tU\tL\tD\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_off_policy_prediction10x10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761ece5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
